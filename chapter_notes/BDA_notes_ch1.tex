\documentclass[a4paper,11pt,english]{article}

\usepackage{babel}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{amsmath}
\usepackage{microtype}

\usepackage[bookmarks=false]{hyperref}
\hypersetup{%
  bookmarksopen=true,
  bookmarksnumbered=true,
  pdftitle={Bayesian data analysis},
  pdfsubject={Reading instructions},
  pdfauthor={Aki Vehtari},
  pdfkeywords={Bayesian probability theory, Bayesian inference, Bayesian data analysis},
  pdfstartview={FitH -32768}
}


% if not draft, smaller printable area makes the paper more readable
\topmargin -4mm
\oddsidemargin 0mm
\textheight 225mm
\textwidth 160mm

%\parskip=\baselineskip

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\Sd}{Sd}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Beta}{Beta}
\DeclareMathOperator{\Invchi2}{Inv-\chi^2}
\DeclareMathOperator{\logit}{logit}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\tr}{tr}
%\DeclareMathOperator{\Pr}{Pr}
\DeclareMathOperator{\trace}{trace}

\pagestyle{empty}

\begin{document}
\thispagestyle{empty}

\section*{Bayesian data analysis -- reading instructions} 
\smallskip
{\bf Aki Vehtari}
\smallskip

\subsection*{Chapter 1 -- outline}

Outline of the chapter 1
\begin{list}{$\bullet$}{\parsep=0pt\itemsep=2pt}
\item 1.1-1.3 important terms, especially 1.3 for the notation
\item 1.4 an example related to the first excerise, and another
  practical example
\item 1.5 foundations
\item 1.6 good example related to visualisation exercise
\item 1.7 example which can be skipped
\item 1.8 background material, good to read before doing the first exercises
\item 1.9 background material, good to read before doing the second exercises
\item 1.10 a point of view for using Bayesian inference
\end{list}

\subsection*{Chapter 1 -- most important terms}

Find all the terms and symbols listed below. Note that some of the
terms are now only briefly introduced and will be covered later in
more detail.
When reading the chapter, write down questions related to things
unclear for you or things you think might be unclear for others.
\begin{list}{$\bullet$}{\parsep=0pt\itemsep=2pt}
\item full probability model
\item posterior distribution
\item potentially observable quantity
\item quantities that are not directly observable
\item exchangeability
\item independently and identically distributed
\item $\theta, y, \tilde{y}, x, X, p(\cdot|\cdot), p(\cdot), \Pr(\cdot), \sim, H$
\item sd, E, var
\item Bayes rule
\item prior distribution
\item sampling distribution, data distribution
\item joint probability distribution
\item posterior density
\item probability
\item density
\item distribution
\item $p(y|\theta)$ as a function of $y$ or $\theta$
\item likelihood
\item posterior predictive distribution
\item probability as measure of uncertainty
\item subjectivity and objectivity
\item transformation of variables
\item simulation
\item inverse cumulative distribution function
\end{list}

\subsection*{Model and likelihood}

Term $p(y|\theta,M)$ has two different names depending on the
situation. Due to the short notation used, there is possibility of
confusion.

\begin{itemize}
\item[1)] Term $p(y|\theta,M)$ is called a \emph{model} (sometimes
  more specifically \emph{observation model} or \emph{statistical
    model}) when it is used to describe uncertainty about $y$ given
  $\theta$ and $M$. Longer notation $p_y(y|\theta,M)$ shows explicitly
  that it is a function of $y$.

\item[2)] In Bayes rule, the term $p(y|\theta,M)$ is called
  \emph{likelihood function}. Posterior distribution describes the
  probability (or probability density) for different values of
  $\theta$ given a fixed $y$, and thus when the posterior is computed
  the terms on the right hand side (in Bayes rule) are also evaluated
  as a function of $\theta$ given fixed $y$. Longer notation
  $p_\theta(y|\theta,M)$ shows explicitly that it is a function of
  $\theta$. Term has it's own name (likelihood) to make the differene
  to the model. The likelihood function is unnormalized probability
  distribution describing uncertainty related to $\theta$ (and that's
  why Bayes rule has the normalization term to get the posterior
  distribution).
\end{itemize}

\subsection*{Two types of uncertainty}

Epistemic and aletory uncertainty are reviewed nicely in the article:
Tony O'Hagan, "Dicing with unknown"
Significance 1(3):132-133, 2004. \url{http://onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2004.00050.x/abstract}

\subsection*{Transformation of variables}

  \begin{itemize}
  \item BDA3 p. 21
  \item Logic, Probability, and Bayesian Inference by Michael Betancourt
    \url{https://github.com/betanalpha/stan_intro/blob/master/stan_intro.pdf}
  \end{itemize}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
